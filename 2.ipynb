{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-18T01:44:32.442345Z",
     "start_time": "2025-10-18T01:44:31.124025Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv( \"data/labeledTrainData.tsv\", delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"data/testData.tsv\", delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"data/unlabeledTrainData.tsv\",delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "print(f\"Read {train['review'].size} labeled train reviews, {test['review'].size} labeled test reviews, and {unlabeled_train['review'].size} unlabeled reviews\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "不移除停用词，学习更广泛的关系特征",
   "id": "3856c0ec8f829794"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T02:29:56.797269Z",
     "start_time": "2025-10-18T02:29:56.790936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "def review_to_wordlist(review,remove_stopwords=False):\n",
    "    # 移除 html标记，转换为纯文本\n",
    "    review_text = BeautifulSoup(review, \"lxml\").text\n",
    "    # 移除数字字符\n",
    "    review_text = re.sub(r\"[^a-zA-Z]\", \" \", review_text)\n",
    "    # 不移除停用词\n",
    "    # 转换成小写\n",
    "    review_text = review_text.lower()\n",
    "    words = review_text.split()\n",
    "    # 停用词\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return words"
   ],
   "id": "52a7bceab1fec122",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "word2vec期望输入的是单个句子，所以要将段落拆分成句子-如何判断句子结束？-nltk-punkt",
   "id": "e78754777a27fdfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:44:39.843627Z",
     "start_time": "2025-10-18T01:44:32.707571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk.data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n"
   ],
   "id": "448d80d2ae212a3f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wangyu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/wangyu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T02:30:04.221296Z",
     "start_time": "2025-10-18T02:30:04.198348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    # review.strip()：先移除文本首尾的空白字符（如空格、换行符等），清理原始文本\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( review_to_wordlist( raw_sentence,remove_stopwords))\n",
    "    return sentences"
   ],
   "id": "7b28e6c0b9cbbf23",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:25.342528Z",
     "start_time": "2025-10-18T01:44:39.947759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "for review in train['review']:\n",
    "    sentences+=review_to_sentences( review, tokenizer )\n",
    "# 如果用 append 会将函数返回的整个数组添加到 sentences 中[]\n",
    "# +=则是将函数返回的数组中的内容添加到 sentences 中\n",
    "for review in unlabeled_train['review']:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n"
   ],
   "id": "71816ac8432fd4e9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/y1sl7z4x3_zcsnqgh6cgv1g00000gn/T/ipykernel_71048/1670871272.py:5: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  review_text = BeautifulSoup(review, \"lxml\").text\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:25.359736Z",
     "start_time": "2025-10-18T01:45:25.356934Z"
    }
   },
   "cell_type": "code",
   "source": "len(sentences)",
   "id": "74a5f76856d1bd23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "796172"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:25.387624Z",
     "start_time": "2025-10-18T01:45:25.384061Z"
    }
   },
   "cell_type": "code",
   "source": "sentences[0]",
   "id": "ab78ab3392171a83",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with',\n",
       " 'all',\n",
       " 'this',\n",
       " 'stuff',\n",
       " 'going',\n",
       " 'down',\n",
       " 'at',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'with',\n",
       " 'mj',\n",
       " 'i',\n",
       " 've',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'to',\n",
       " 'his',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'the',\n",
       " 'odd',\n",
       " 'documentary',\n",
       " 'here',\n",
       " 'and',\n",
       " 'there',\n",
       " 'watched',\n",
       " 'the',\n",
       " 'wiz',\n",
       " 'and',\n",
       " 'watched',\n",
       " 'moonwalker',\n",
       " 'again']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:58.709858Z",
     "start_time": "2025-10-18T01:45:25.400721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "num_features = 300    # 词向量维度\n",
    "min_word_count = 40   # 最小词数，丢弃 min_count 以下词（节省内存、去噪）。\n",
    "num_workers = 4       # 并行运行的线程数量\n",
    "context = 10          # 上下文窗口大小\n",
    "downsampling = 1e-3   # 高频词的下采样设置，对高频词按概率丢弃部分出现，减少“the / , / .”的干扰（sample 参数）。\n",
    "\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "# 用 negative sampling，假设抽到负样 dog, apple，目标是最大化：\n",
    "# log σ(v_cat·v_say) + log σ(v_meow·v_say) + log σ(-v_dog·v_say) + log σ(-v_apple·v_say)"
   ],
   "id": "462e0ceaccc89491",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 09:45:25,550 : INFO : collecting all words and their counts\n",
      "2025-10-18 09:45:25,551 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-10-18 09:45:25,567 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n",
      "2025-10-18 09:45:25,585 : INFO : PROGRESS: at sentence #20000, processed 451738 words, keeping 24945 word types\n",
      "2025-10-18 09:45:25,602 : INFO : PROGRESS: at sentence #30000, processed 670859 words, keeping 30027 word types\n",
      "2025-10-18 09:45:25,620 : INFO : PROGRESS: at sentence #40000, processed 896841 words, keeping 34335 word types\n",
      "2025-10-18 09:45:25,638 : INFO : PROGRESS: at sentence #50000, processed 1116082 words, keeping 37751 word types\n",
      "2025-10-18 09:45:25,655 : INFO : PROGRESS: at sentence #60000, processed 1337544 words, keeping 40711 word types\n",
      "2025-10-18 09:45:25,672 : INFO : PROGRESS: at sentence #70000, processed 1560307 words, keeping 43311 word types\n",
      "2025-10-18 09:45:25,689 : INFO : PROGRESS: at sentence #80000, processed 1779516 words, keeping 45707 word types\n",
      "2025-10-18 09:45:25,707 : INFO : PROGRESS: at sentence #90000, processed 2003714 words, keeping 48121 word types\n",
      "2025-10-18 09:45:25,724 : INFO : PROGRESS: at sentence #100000, processed 2225465 words, keeping 50190 word types\n",
      "2025-10-18 09:45:25,742 : INFO : PROGRESS: at sentence #110000, processed 2444323 words, keeping 52058 word types\n",
      "2025-10-18 09:45:25,773 : INFO : PROGRESS: at sentence #120000, processed 2666488 words, keeping 54098 word types\n",
      "2025-10-18 09:45:25,791 : INFO : PROGRESS: at sentence #130000, processed 2892315 words, keeping 55837 word types\n",
      "2025-10-18 09:45:25,809 : INFO : PROGRESS: at sentence #140000, processed 3104796 words, keeping 57324 word types\n",
      "2025-10-18 09:45:25,827 : INFO : PROGRESS: at sentence #150000, processed 3330432 words, keeping 59045 word types\n",
      "2025-10-18 09:45:25,845 : INFO : PROGRESS: at sentence #160000, processed 3552466 words, keeping 60581 word types\n",
      "2025-10-18 09:45:25,863 : INFO : PROGRESS: at sentence #170000, processed 3776048 words, keeping 62050 word types\n",
      "2025-10-18 09:45:25,879 : INFO : PROGRESS: at sentence #180000, processed 3996237 words, keeping 63483 word types\n",
      "2025-10-18 09:45:25,896 : INFO : PROGRESS: at sentence #190000, processed 4221288 words, keeping 64775 word types\n",
      "2025-10-18 09:45:25,914 : INFO : PROGRESS: at sentence #200000, processed 4445973 words, keeping 66070 word types\n",
      "2025-10-18 09:45:25,931 : INFO : PROGRESS: at sentence #210000, processed 4666511 words, keeping 67367 word types\n",
      "2025-10-18 09:45:25,949 : INFO : PROGRESS: at sentence #220000, processed 4892037 words, keeping 68686 word types\n",
      "2025-10-18 09:45:25,966 : INFO : PROGRESS: at sentence #230000, processed 5113881 words, keeping 69935 word types\n",
      "2025-10-18 09:45:25,984 : INFO : PROGRESS: at sentence #240000, processed 5340847 words, keeping 71144 word types\n",
      "2025-10-18 09:45:26,001 : INFO : PROGRESS: at sentence #250000, processed 5555463 words, keeping 72333 word types\n",
      "2025-10-18 09:45:26,016 : INFO : PROGRESS: at sentence #260000, processed 5775304 words, keeping 73466 word types\n",
      "2025-10-18 09:45:26,033 : INFO : PROGRESS: at sentence #270000, processed 5995572 words, keeping 74740 word types\n",
      "2025-10-18 09:45:26,050 : INFO : PROGRESS: at sentence #280000, processed 6220911 words, keeping 76318 word types\n",
      "2025-10-18 09:45:26,067 : INFO : PROGRESS: at sentence #290000, processed 6443523 words, keeping 77787 word types\n",
      "2025-10-18 09:45:26,084 : INFO : PROGRESS: at sentence #300000, processed 6668258 words, keeping 79142 word types\n",
      "2025-10-18 09:45:26,101 : INFO : PROGRESS: at sentence #310000, processed 6892662 words, keeping 80431 word types\n",
      "2025-10-18 09:45:26,119 : INFO : PROGRESS: at sentence #320000, processed 7118969 words, keeping 81794 word types\n",
      "2025-10-18 09:45:26,136 : INFO : PROGRESS: at sentence #330000, processed 7340486 words, keeping 83006 word types\n",
      "2025-10-18 09:45:26,153 : INFO : PROGRESS: at sentence #340000, processed 7569986 words, keeping 84252 word types\n",
      "2025-10-18 09:45:26,171 : INFO : PROGRESS: at sentence #350000, processed 7792927 words, keeping 85407 word types\n",
      "2025-10-18 09:45:26,189 : INFO : PROGRESS: at sentence #360000, processed 8012526 words, keeping 86567 word types\n",
      "2025-10-18 09:45:26,208 : INFO : PROGRESS: at sentence #370000, processed 8239772 words, keeping 87663 word types\n",
      "2025-10-18 09:45:26,226 : INFO : PROGRESS: at sentence #380000, processed 8465827 words, keeping 88849 word types\n",
      "2025-10-18 09:45:26,244 : INFO : PROGRESS: at sentence #390000, processed 8694607 words, keeping 89883 word types\n",
      "2025-10-18 09:45:26,262 : INFO : PROGRESS: at sentence #400000, processed 8917820 words, keeping 90882 word types\n",
      "2025-10-18 09:45:26,280 : INFO : PROGRESS: at sentence #410000, processed 9138504 words, keeping 91859 word types\n",
      "2025-10-18 09:45:26,298 : INFO : PROGRESS: at sentence #420000, processed 9358474 words, keeping 92880 word types\n",
      "2025-10-18 09:45:26,317 : INFO : PROGRESS: at sentence #430000, processed 9586958 words, keeping 93909 word types\n",
      "2025-10-18 09:45:26,333 : INFO : PROGRESS: at sentence #440000, processed 9812576 words, keeping 94853 word types\n",
      "2025-10-18 09:45:26,352 : INFO : PROGRESS: at sentence #450000, processed 10036719 words, keeping 95995 word types\n",
      "2025-10-18 09:45:26,369 : INFO : PROGRESS: at sentence #460000, processed 10269931 words, keeping 97064 word types\n",
      "2025-10-18 09:45:26,403 : INFO : PROGRESS: at sentence #470000, processed 10496262 words, keeping 97885 word types\n",
      "2025-10-18 09:45:26,420 : INFO : PROGRESS: at sentence #480000, processed 10717170 words, keeping 98809 word types\n",
      "2025-10-18 09:45:26,439 : INFO : PROGRESS: at sentence #490000, processed 10943335 words, keeping 99835 word types\n",
      "2025-10-18 09:45:26,458 : INFO : PROGRESS: at sentence #500000, processed 11165141 words, keeping 100726 word types\n",
      "2025-10-18 09:45:26,479 : INFO : PROGRESS: at sentence #510000, processed 11390498 words, keeping 101672 word types\n",
      "2025-10-18 09:45:26,497 : INFO : PROGRESS: at sentence #520000, processed 11613511 words, keeping 102557 word types\n",
      "2025-10-18 09:45:26,518 : INFO : PROGRESS: at sentence #530000, processed 11838774 words, keeping 103374 word types\n",
      "2025-10-18 09:45:26,538 : INFO : PROGRESS: at sentence #540000, processed 12062185 words, keeping 104231 word types\n",
      "2025-10-18 09:45:26,555 : INFO : PROGRESS: at sentence #550000, processed 12286959 words, keeping 105098 word types\n",
      "2025-10-18 09:45:26,575 : INFO : PROGRESS: at sentence #560000, processed 12509034 words, keeping 105971 word types\n",
      "2025-10-18 09:45:26,593 : INFO : PROGRESS: at sentence #570000, processed 12736827 words, keeping 106757 word types\n",
      "2025-10-18 09:45:26,615 : INFO : PROGRESS: at sentence #580000, processed 12958427 words, keeping 107611 word types\n",
      "2025-10-18 09:45:26,632 : INFO : PROGRESS: at sentence #590000, processed 13184325 words, keeping 108468 word types\n",
      "2025-10-18 09:45:26,650 : INFO : PROGRESS: at sentence #600000, processed 13406551 words, keeping 109189 word types\n",
      "2025-10-18 09:45:26,665 : INFO : PROGRESS: at sentence #610000, processed 13628198 words, keeping 110055 word types\n",
      "2025-10-18 09:45:26,683 : INFO : PROGRESS: at sentence #620000, processed 13852588 words, keeping 110805 word types\n",
      "2025-10-18 09:45:26,702 : INFO : PROGRESS: at sentence #630000, processed 14075901 words, keeping 111573 word types\n",
      "2025-10-18 09:45:26,720 : INFO : PROGRESS: at sentence #640000, processed 14298046 words, keeping 112386 word types\n",
      "2025-10-18 09:45:26,739 : INFO : PROGRESS: at sentence #650000, processed 14522874 words, keeping 113151 word types\n",
      "2025-10-18 09:45:26,757 : INFO : PROGRESS: at sentence #660000, processed 14745445 words, keeping 113890 word types\n",
      "2025-10-18 09:45:26,776 : INFO : PROGRESS: at sentence #670000, processed 14970569 words, keeping 114613 word types\n",
      "2025-10-18 09:45:26,795 : INFO : PROGRESS: at sentence #680000, processed 15194625 words, keeping 115331 word types\n",
      "2025-10-18 09:45:26,815 : INFO : PROGRESS: at sentence #690000, processed 15416773 words, keeping 116099 word types\n",
      "2025-10-18 09:45:26,833 : INFO : PROGRESS: at sentence #700000, processed 15645695 words, keeping 116902 word types\n",
      "2025-10-18 09:45:26,851 : INFO : PROGRESS: at sentence #710000, processed 15865815 words, keeping 117541 word types\n",
      "2025-10-18 09:45:26,870 : INFO : PROGRESS: at sentence #720000, processed 16093342 words, keeping 118183 word types\n",
      "2025-10-18 09:45:26,889 : INFO : PROGRESS: at sentence #730000, processed 16316787 words, keeping 118912 word types\n",
      "2025-10-18 09:45:26,908 : INFO : PROGRESS: at sentence #740000, processed 16539147 words, keeping 119618 word types\n",
      "2025-10-18 09:45:26,924 : INFO : PROGRESS: at sentence #750000, processed 16758552 words, keeping 120264 word types\n",
      "2025-10-18 09:45:26,943 : INFO : PROGRESS: at sentence #760000, processed 16977111 words, keeping 120888 word types\n",
      "2025-10-18 09:45:26,962 : INFO : PROGRESS: at sentence #770000, processed 17203259 words, keeping 121656 word types\n",
      "2025-10-18 09:45:26,980 : INFO : PROGRESS: at sentence #780000, processed 17432844 words, keeping 122358 word types\n",
      "2025-10-18 09:45:26,999 : INFO : PROGRESS: at sentence #790000, processed 17660151 words, keeping 123033 word types\n",
      "2025-10-18 09:45:27,011 : INFO : collected 123504 word types from a corpus of 17798270 raw words and 796172 sentences\n",
      "2025-10-18 09:45:27,012 : INFO : Creating a fresh vocabulary\n",
      "2025-10-18 09:45:27,043 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16490 unique words (13.35% of original 123504, drops 107014)', 'datetime': '2025-10-18T09:45:27.043118', 'gensim': '4.3.2', 'python': '3.10.18 (main, Jun  5 2025, 08:37:47) [Clang 14.0.6 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-10-18 09:45:27,043 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 17239125 word corpus (96.86% of original 17798270, drops 559145)', 'datetime': '2025-10-18T09:45:27.043508', 'gensim': '4.3.2', 'python': '3.10.18 (main, Jun  5 2025, 08:37:47) [Clang 14.0.6 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-10-18 09:45:27,067 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2025-10-18 09:45:27,068 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2025-10-18 09:45:27,069 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12749798.434354488 word corpus (74.0%% of prior 17239125)', 'datetime': '2025-10-18T09:45:27.069032', 'gensim': '4.3.2', 'python': '3.10.18 (main, Jun  5 2025, 08:37:47) [Clang 14.0.6 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2025-10-18 09:45:27,107 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2025-10-18 09:45:27,107 : INFO : resetting layer weights\n",
      "2025-10-18 09:45:27,120 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-18T09:45:27.120299', 'gensim': '4.3.2', 'python': '3.10.18 (main, Jun  5 2025, 08:37:47) [Clang 14.0.6 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2025-10-18 09:45:27,120 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2025-10-18T09:45:27.120657', 'gensim': '4.3.2', 'python': '3.10.18 (main, Jun  5 2025, 08:37:47) [Clang 14.0.6 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2025-10-18 09:45:28,126 : INFO : EPOCH 0 - PROGRESS: at 16.23% examples, 2052806 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:29,129 : INFO : EPOCH 0 - PROGRESS: at 33.88% examples, 2137003 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:30,134 : INFO : EPOCH 0 - PROGRESS: at 50.85% examples, 2146520 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:31,134 : INFO : EPOCH 0 - PROGRESS: at 68.04% examples, 2159971 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:32,139 : INFO : EPOCH 0 - PROGRESS: at 84.72% examples, 2152511 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:33,096 : INFO : EPOCH 0: training on 17798270 raw words (12751217 effective words) took 6.0s, 2134589 effective words/s\n",
      "2025-10-18 09:45:34,106 : INFO : EPOCH 1 - PROGRESS: at 16.55% examples, 2079197 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:35,111 : INFO : EPOCH 1 - PROGRESS: at 32.23% examples, 2027708 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:36,113 : INFO : EPOCH 1 - PROGRESS: at 48.43% examples, 2041812 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-18 09:45:37,113 : INFO : EPOCH 1 - PROGRESS: at 64.79% examples, 2054970 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:38,117 : INFO : EPOCH 1 - PROGRESS: at 80.82% examples, 2051440 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:39,122 : INFO : EPOCH 1 - PROGRESS: at 97.18% examples, 2055923 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:39,291 : INFO : EPOCH 1: training on 17798270 raw words (12750293 effective words) took 6.2s, 2058726 effective words/s\n",
      "2025-10-18 09:45:40,294 : INFO : EPOCH 2 - PROGRESS: at 15.50% examples, 1965042 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-18 09:45:41,302 : INFO : EPOCH 2 - PROGRESS: at 31.37% examples, 1978871 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:42,310 : INFO : EPOCH 2 - PROGRESS: at 47.56% examples, 2002232 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:43,315 : INFO : EPOCH 2 - PROGRESS: at 63.55% examples, 2012151 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:44,324 : INFO : EPOCH 2 - PROGRESS: at 79.29% examples, 2008123 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:45,325 : INFO : EPOCH 2 - PROGRESS: at 95.00% examples, 2007161 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:45,664 : INFO : EPOCH 2: training on 17798270 raw words (12749967 effective words) took 6.4s, 2001136 effective words/s\n",
      "2025-10-18 09:45:46,667 : INFO : EPOCH 3 - PROGRESS: at 15.56% examples, 1972671 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:47,671 : INFO : EPOCH 3 - PROGRESS: at 30.73% examples, 1942827 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:48,674 : INFO : EPOCH 3 - PROGRESS: at 46.11% examples, 1946211 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:49,674 : INFO : EPOCH 3 - PROGRESS: at 61.58% examples, 1956392 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:50,678 : INFO : EPOCH 3 - PROGRESS: at 77.16% examples, 1961555 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:51,678 : INFO : EPOCH 3 - PROGRESS: at 92.79% examples, 1967156 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-18 09:45:52,154 : INFO : EPOCH 3: training on 17798270 raw words (12747812 effective words) took 6.5s, 1964784 effective words/s\n",
      "2025-10-18 09:45:53,162 : INFO : EPOCH 4 - PROGRESS: at 15.62% examples, 1969234 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:54,165 : INFO : EPOCH 4 - PROGRESS: at 30.97% examples, 1953982 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:55,175 : INFO : EPOCH 4 - PROGRESS: at 46.11% examples, 1939025 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-18 09:45:56,182 : INFO : EPOCH 4 - PROGRESS: at 61.64% examples, 1949466 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:57,183 : INFO : EPOCH 4 - PROGRESS: at 77.04% examples, 1952736 words/s, in_qsize 8, out_qsize 0\n",
      "2025-10-18 09:45:58,194 : INFO : EPOCH 4 - PROGRESS: at 92.56% examples, 1953974 words/s, in_qsize 7, out_qsize 0\n",
      "2025-10-18 09:45:58,708 : INFO : EPOCH 4: training on 17798270 raw words (12748597 effective words) took 6.6s, 1945867 effective words/s\n",
      "2025-10-18 09:45:58,708 : INFO : Word2Vec lifecycle event {'msg': 'training on 88991350 raw words (63747886 effective words) took 31.6s, 2018136 effective words/s', 'datetime': '2025-10-18T09:45:58.708612', 'gensim': '4.3.2', 'python': '3.10.18 (main, Jun  5 2025, 08:37:47) [Clang 14.0.6 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2025-10-18 09:45:58,708 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16490, vector_size=300, alpha=0.025>', 'datetime': '2025-10-18T09:45:58.708851', 'gensim': '4.3.2', 'python': '3.10.18 (main, Jun  5 2025, 08:37:47) [Clang 14.0.6 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:58.796240Z",
     "start_time": "2025-10-18T01:45:58.788720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 对模型的词向量进行 归一化（normalization），并通过 replace=True 参数来 节省内存（覆盖原始向量)，但是已经过时\n",
    "# 归一化让“相似度”真正代表语义，而不是受词频或向量长度干扰。\n",
    "model.wv.fill_norms()  # 计算并缓存所有词向量的L2范数"
   ],
   "id": "152f3c78b008e4a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:58.833429Z",
     "start_time": "2025-10-18T01:45:58.813590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"300features_40minwords_10context.model\"\n",
    "model.save(model_name)\n"
   ],
   "id": "3beb2b374e80dbc3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 09:45:58,814 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-10-18T09:45:58.814170', 'gensim': '4.3.2', 'python': '3.10.18 (main, Jun  5 2025, 08:37:47) [Clang 14.0.6 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'saving'}\n",
      "2025-10-18 09:45:58,814 : INFO : not storing attribute cum_table\n",
      "2025-10-18 09:45:58,832 : INFO : saved 300features_40minwords_10context.model\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:58.882168Z",
     "start_time": "2025-10-18T01:45:58.878296Z"
    }
   },
   "cell_type": "code",
   "source": "model.wv.doesnt_match(\"man woman child kitchen\".split())",
   "id": "d9a008db63af1391",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:58.949792Z",
     "start_time": "2025-10-18T01:45:58.945477Z"
    }
   },
   "cell_type": "code",
   "source": "model.wv.doesnt_match(\"france england germany berlin\".split())",
   "id": "5c4ffcaae2e1d4b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:59.030299Z",
     "start_time": "2025-10-18T01:45:59.024304Z"
    }
   },
   "cell_type": "code",
   "source": "model.wv.doesnt_match(\"paris berlin london austria\".split())",
   "id": "1affc5fdd6cce28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'austria'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:59.134592Z",
     "start_time": "2025-10-18T01:45:59.108442Z"
    }
   },
   "cell_type": "code",
   "source": "model.wv.most_similar(\"man\")",
   "id": "fcaef344274f9caf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.5995623469352722),\n",
       " ('lady', 0.5918226838111877),\n",
       " ('lad', 0.5749083161354065),\n",
       " ('men', 0.5204932689666748),\n",
       " ('chap', 0.5144463181495667),\n",
       " ('guy', 0.5101121068000793),\n",
       " ('bloke', 0.507328450679779),\n",
       " ('doctor', 0.4993882477283478),\n",
       " ('farmer', 0.49911344051361084),\n",
       " ('monk', 0.49789494276046753)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:59.359487Z",
     "start_time": "2025-10-18T01:45:59.353538Z"
    }
   },
   "cell_type": "code",
   "source": "model.wv.most_similar(\"queen\")",
   "id": "842f3368852012ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6741946339607239),\n",
       " ('bride', 0.6582250595092773),\n",
       " ('victoria', 0.6034635305404663),\n",
       " ('latifah', 0.603192925453186),\n",
       " ('showgirl', 0.5710378885269165),\n",
       " ('stepmother', 0.5692368745803833),\n",
       " ('maid', 0.5689412355422974),\n",
       " ('prince', 0.5601257085800171),\n",
       " ('mistress', 0.5599981546401978),\n",
       " ('belle', 0.5588557720184326)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:59.499563Z",
     "start_time": "2025-10-18T01:45:59.492057Z"
    }
   },
   "cell_type": "code",
   "source": "model.wv.most_similar(\"awful\")",
   "id": "9b2358cfb874213b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.760650634765625),\n",
       " ('horrible', 0.7319118976593018),\n",
       " ('atrocious', 0.7287877202033997),\n",
       " ('abysmal', 0.7120822668075562),\n",
       " ('dreadful', 0.6972820162773132),\n",
       " ('appalling', 0.6770805716514587),\n",
       " ('horrendous', 0.6694055199623108),\n",
       " ('horrid', 0.6627129912376404),\n",
       " ('lousy', 0.6256157159805298),\n",
       " ('amateurish', 0.5981056094169617)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## part_3",
   "id": "84f04b1fca2fe7c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:59.706200Z",
     "start_time": "2025-10-18T01:45:59.591392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context.model\")"
   ],
   "id": "1cc3bfdae391feb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 09:45:59,592 : INFO : loading Word2Vec object from 300features_40minwords_10context.model\n",
      "2025-10-18 09:45:59,608 : INFO : loading wv recursively from 300features_40minwords_10context.model.wv.* with mmap=None\n",
      "2025-10-18 09:45:59,612 : INFO : setting ignored attribute cum_table to None\n",
      "2025-10-18 09:45:59,691 : INFO : Word2Vec lifecycle event {'fname': '300features_40minwords_10context.model', 'datetime': '2025-10-18T09:45:59.691445', 'gensim': '4.3.2', 'python': '3.10.18 (main, Jun  5 2025, 08:37:47) [Clang 14.0.6 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:59.800555Z",
     "start_time": "2025-10-18T01:45:59.798207Z"
    }
   },
   "cell_type": "code",
   "source": "model.wv.vectors",
   "id": "b0aac647e4a3b560",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.46203578, -1.1892741 ,  0.41406223, ...,  1.8321238 ,\n",
       "        -0.32884738,  1.3248438 ],\n",
       "       [-0.64237416, -0.9830256 ,  0.7323948 , ...,  0.28813797,\n",
       "        -0.59826034, -0.01477962],\n",
       "       [ 2.3623862 , -0.33042914, -0.33293054, ...,  0.11848633,\n",
       "         0.7027185 ,  0.8033076 ],\n",
       "       ...,\n",
       "       [ 0.2277323 ,  0.17956766,  0.073678  , ..., -0.09059525,\n",
       "         0.10255343, -0.00902587],\n",
       "       [-0.03246737,  0.10530712, -0.09102537, ...,  0.1781935 ,\n",
       "         0.01185587, -0.08071371],\n",
       "       [-0.12659827,  0.0107701 , -0.17256647, ..., -0.18027577,\n",
       "        -0.17464   , -0.10304833]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T01:45:59.897247Z",
     "start_time": "2025-10-18T01:45:59.895585Z"
    }
   },
   "cell_type": "code",
   "source": "model.wv.vectors.shape",
   "id": "4236bf9f53ffbdb5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T02:10:28.745766Z",
     "start_time": "2025-10-18T02:10:28.741543Z"
    }
   },
   "cell_type": "code",
   "source": "type(model.wv.vectors)",
   "id": "237bc5ca0a59ac08",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T02:11:03.196661Z",
     "start_time": "2025-10-18T02:11:03.191884Z"
    }
   },
   "cell_type": "code",
   "source": "model.wv[\"flower\"]",
   "id": "a7203f6f86ba857c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.45072499e-01,  2.62092233e-01,  1.96153913e-02,  2.21510842e-01,\n",
       "       -3.40615883e-02,  2.07704619e-01, -2.44022463e-03,  3.67342085e-01,\n",
       "        4.79519516e-01, -5.22090137e-01,  1.62662044e-01,  4.31865007e-01,\n",
       "        1.58299997e-01, -8.22339579e-02, -2.61161238e-01,  5.76466545e-02,\n",
       "       -2.00415507e-01, -5.80791295e-01, -4.38209385e-01, -1.05509900e-01,\n",
       "        1.09054632e-01,  9.70642269e-02,  2.02213839e-01,  1.14739798e-01,\n",
       "        4.44840193e-01, -4.28412156e-03,  1.56124271e-02, -1.40784860e-01,\n",
       "       -1.11332096e-01,  9.98667181e-02,  4.90062475e-01, -5.52447028e-02,\n",
       "       -1.60927176e-01, -3.06963831e-01, -2.41093040e-01, -2.55013764e-01,\n",
       "        3.18381071e-01, -7.32264280e-01,  1.98628649e-01, -1.94106191e-01,\n",
       "        1.91836730e-02,  1.97823420e-01,  3.75492662e-01,  2.20785379e-01,\n",
       "       -3.21409702e-01, -1.26897497e-02,  2.29085654e-01,  4.98462379e-01,\n",
       "        5.01539767e-01,  4.51061308e-01,  5.84751070e-01,  3.36822242e-01,\n",
       "       -3.34707379e-01,  2.06882328e-01, -3.17501053e-02,  1.14991732e-01,\n",
       "        6.50037169e-01,  6.88821003e-02, -7.24703431e-01, -5.53609729e-02,\n",
       "       -5.79573750e-01,  3.72922629e-01,  2.22847417e-01,  2.93092281e-01,\n",
       "       -4.15129930e-01,  2.64713973e-01, -3.39412600e-01,  3.00554633e-01,\n",
       "       -1.07150361e-01, -1.09676503e-01,  1.83432236e-01,  4.22259778e-01,\n",
       "        4.05348927e-01, -1.78341106e-01, -2.99514592e-01,  9.13903862e-02,\n",
       "        1.04687318e-01,  1.55728787e-01, -7.78971538e-02,  7.84849226e-02,\n",
       "       -1.15070874e-02,  9.35225189e-02,  6.40388429e-02,  3.79691720e-01,\n",
       "        2.25493282e-01,  7.51769990e-02, -3.57952774e-01,  1.02059938e-01,\n",
       "       -9.58122686e-02, -1.37017891e-01, -2.33331453e-02,  2.98037410e-01,\n",
       "       -3.70749086e-01,  2.95468241e-01,  1.50916219e-01,  1.01056412e-01,\n",
       "        1.62184775e-01,  3.06067377e-01, -7.00064600e-01,  2.11309105e-01,\n",
       "        2.93984264e-01,  8.92040730e-01,  2.04341412e-01,  1.10980943e-01,\n",
       "        6.73285425e-01,  2.85451263e-01, -6.24068156e-02, -2.22983360e-01,\n",
       "        2.34440431e-01, -1.22957909e-02,  5.05942740e-02,  4.52807248e-01,\n",
       "        2.77622819e-01,  2.95950592e-01,  3.85134667e-01, -1.98796123e-01,\n",
       "        4.56140369e-01,  3.15999061e-01,  3.35648991e-02, -4.92385566e-01,\n",
       "       -1.44865841e-01,  1.19867720e-01, -3.97392921e-02, -1.23439960e-01,\n",
       "       -1.61041304e-01,  1.07217044e-01,  4.03533243e-02, -5.58554590e-01,\n",
       "        1.54355660e-01,  4.32298630e-01, -5.80563992e-02,  4.19502139e-01,\n",
       "        5.78196645e-02, -3.36588532e-01,  2.89004236e-01, -7.17424080e-02,\n",
       "       -2.15632617e-01, -3.02449241e-03, -4.09046590e-01, -4.03739452e-01,\n",
       "        6.93560764e-02, -3.38321447e-01, -3.68568003e-01, -7.41949677e-02,\n",
       "        2.98122913e-02,  1.39097050e-02, -6.56450570e-01, -6.01983786e-01,\n",
       "        3.63717645e-01,  2.36137390e-01,  1.34932145e-01, -9.11003649e-01,\n",
       "       -1.01328897e+00, -6.01247288e-02,  3.31816785e-02,  9.94023830e-02,\n",
       "       -1.31808504e-01,  8.45163537e-04, -3.01919222e-01,  9.06139135e-01,\n",
       "       -2.89958924e-01, -7.23570138e-02, -3.39187801e-01,  7.15461850e-01,\n",
       "       -1.12985030e-01, -1.57155901e-01,  3.56349409e-01, -4.67231601e-01,\n",
       "        2.26093650e-01,  2.93375611e-01, -1.72526449e-01, -2.63537735e-01,\n",
       "       -1.14767425e-01,  3.20535600e-01, -1.22385286e-01, -4.84499693e-01,\n",
       "        1.96987942e-01, -3.66953939e-01, -2.74258107e-01,  1.22495435e-01,\n",
       "        1.06669001e-01,  2.49328554e-01,  1.93114251e-01, -3.87477189e-01,\n",
       "       -1.50534436e-01, -3.56723815e-01,  2.27422029e-01,  3.99787873e-01,\n",
       "        5.43303788e-01, -1.56269789e-01,  2.13538289e-01,  9.86004248e-02,\n",
       "       -3.90730441e-01, -9.32229087e-02,  1.95378587e-01,  1.29108146e-01,\n",
       "        2.45749518e-01,  1.89753279e-01,  3.71956915e-01, -2.40984082e-01,\n",
       "       -2.91005015e-01,  6.02553248e-01,  1.89019591e-01, -1.23377495e-01,\n",
       "       -5.54832458e-01,  2.81713307e-01, -5.51484562e-02,  2.15165094e-01,\n",
       "       -6.23224787e-02,  9.27910060e-02,  4.14458327e-02, -1.04038336e-03,\n",
       "        2.74370492e-01, -1.74456276e-02,  4.56319116e-02,  2.84118831e-01,\n",
       "       -4.49592978e-01, -5.18148839e-01,  1.63805783e-01,  1.02510378e-01,\n",
       "        6.57625377e-01,  2.95187265e-01,  9.62121189e-02,  3.29821080e-01,\n",
       "       -3.35219920e-01,  2.98571754e-02, -1.48637116e-01, -3.60199846e-02,\n",
       "       -1.95559189e-01,  3.94245207e-01,  1.87997341e-01, -1.39796153e-01,\n",
       "       -1.55858487e-01, -2.40279198e-01,  9.00487453e-02,  9.47827697e-02,\n",
       "       -9.41096842e-02,  2.93902338e-01, -2.06885919e-01, -4.33306426e-01,\n",
       "        7.83482566e-02, -5.15043020e-01, -2.52291799e-01, -2.71848708e-01,\n",
       "        5.22106364e-02,  6.06418140e-02,  1.01786047e-01,  2.11520121e-01,\n",
       "       -2.75977820e-01,  2.10870326e-01,  7.77696865e-03, -1.31079614e-01,\n",
       "        4.39211249e-01,  3.14251065e-01, -5.40474653e-01, -4.66884732e-01,\n",
       "        2.59702146e-01, -3.79526168e-02, -2.63254732e-01,  1.49276897e-01,\n",
       "        1.95020571e-01, -4.23057042e-02, -1.74532235e-01, -2.36555710e-01,\n",
       "       -9.51004326e-02, -7.04049617e-02,  4.64432687e-01,  3.09454530e-01,\n",
       "        9.07713771e-02,  2.71905810e-01, -2.85868198e-01,  5.25072694e-01,\n",
       "       -1.90030947e-01, -4.50633228e-01,  2.76453495e-01, -1.03584260e-01,\n",
       "       -5.36556467e-02, -1.13373406e-01, -4.88961995e-01, -9.36125442e-02,\n",
       "        3.78202857e-03, -2.37439498e-01, -1.04619041e-01,  1.43167794e-01,\n",
       "        2.65926510e-01, -4.71182056e-02,  7.00914189e-02,  3.86850744e-01,\n",
       "        1.62245780e-01,  1.11647420e-01, -3.68796259e-01,  1.28604367e-01,\n",
       "        5.01165867e-01,  1.59202125e-02,  1.05420455e-01,  2.51669645e-01,\n",
       "       -4.87487704e-01, -1.83040574e-01,  3.55038792e-01, -5.04624806e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T02:52:10.674120Z",
     "start_time": "2025-10-18T02:52:10.669382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    # 转换为集合提高速度\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    # 加和取平均\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords+=1\n",
    "            featureVec = np.add(featureVec,model.wv[word])\n",
    "\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    # 二维数组\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "       if counter%1000== 0:\n",
    "           print(f\"Review {counter} of {len(reviews)}\")\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "       counter = counter + 1\n",
    "\n",
    "    return reviewFeatureVecs"
   ],
   "id": "9ccfd5deafc0b7a4",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T02:53:19.078897Z",
     "start_time": "2025-10-18T02:52:12.839777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ],
   "id": "391dd2ce89bb5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T02:58:49.030496Z",
     "start_time": "2025-10-18T02:58:19.264499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
    "\n",
    "# Test & extract results\n",
    "result = forest.predict( testDataVecs )\n",
    "\n",
    "# Write the test results\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )\n",
    "# 可以尝试tf-idf 加权"
   ],
   "id": "5a7337bce95a6dba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "k-means聚类",
   "id": "c7724d6c7789f248"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T07:39:42.373275Z",
     "start_time": "2025-10-18T07:38:56.496941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "word_vectors = model.wv.vectors\n",
    "num_clusters = int(word_vectors.shape[0] / 5)\n",
    "\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ],
   "id": "d61a58f4481d270b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  45.873730182647705 seconds.\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T07:43:25.411789Z",
     "start_time": "2025-10-18T07:43:25.407877Z"
    }
   },
   "cell_type": "code",
   "source": "word_centroid_map = dict(zip( model.wv.index_to_key, idx ))",
   "id": "2dc770f2cf64ef3f",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T07:45:14.628868Z",
     "start_time": "2025-10-18T07:45:14.538193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in range(0,10):\n",
    "    print (f\"Cluster {cluster}\")\n",
    "    words = []\n",
    "    for word, cluster_id in word_centroid_map.items():\n",
    "        if cluster_id == cluster:\n",
    "            words.append(word)\n",
    "    print(words)"
   ],
   "id": "c4ea4b35618f2431",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "['ensues', 'courtroom', 'unexplained', 'resolved', 'abruptly', 'inexplicable', 'rapidly', 'unanswered', 'solved', 'complications', 'awry', 'ensue', 'unravel', 'arise', 'solving', 'occurring', 'unresolved']\n",
      "Cluster 1\n",
      "['headed', 'jerk', 'loser', 'drunken', 'wannabe', 'bumbling', 'macho', 'horny', 'nerd', 'mute', 'bully', 'nut', 'brat', 'token', 'geek', 'biker', 'pimp', 'redneck', 'slimy', 'thug', 'bald', 'puppy', 'midget', 'jock', 'bastard', 'hunk', 'nerdy', 'overweight', 'goth', 'bimbo', 'butch', 'frat', 'cheerleader', 'geeky']\n",
      "Cluster 2\n",
      "['precious', 'failing', 'limit', 'akin', 'ensure', 'enhance', 'qualify', 'sustain', 'warrant', 'salvage', 'compensate', 'increase', 'breathe', 'analyze', 'emphasize', 'retain', 'tackle', 'invest', 'recreate', 'imitate', 'elevate', 'expand', 'indulge', 'muster', 'digest', 'emulate', 'render', 'adjust', 'provoke', 'elicit', 'recapture', 'induce', 'extend', 'undermine', 'absorb', 'reduce', 'replicate']\n",
      "Cluster 3\n",
      "['demand', 'grip', 'click', 'tables', 'spotlight', 'boards', 'outlook', 'horizon', 'insisted', 'brink', 'forum', 'downward', 'inflicted', 'ju', 'canvas', 'imposed', 'cheats', 'meditation', 'linger', 'lingers', 'printed', 'rests', 'insistence', 'whim', 'embarks', 'dwell', 'spying', 'orient', 'tbs', 'fixation', 'bandwagon', 'doorstep', 'inflict', 'drone', 'hinges', 'thrive', 'sheds', 'ramble', 'lookout']\n",
      "Cluster 4\n",
      "['displays', 'demonstrates']\n",
      "Cluster 5\n",
      "['position', 'ego']\n",
      "Cluster 6\n",
      "['divine', 'raj', 'akbar', 'amelie', 'madhuri', 'preity', 'aishwarya', 'madly', 'hrithik', 'shahid', 'amrita', 'rai', 'zinta', 'jodhaa', 'juhi', 'winona', 'oberoi', 'roshan', 'tautou', 'sharma', 'naina', 'ranbir', 'deol', 'srk', 'mukherjee']\n",
      "Cluster 7\n",
      "['rex', 'sh']\n",
      "Cluster 8\n",
      "['existed', 'witnessed', 'guessed', 'misfortune', 'encountered', 'na']\n",
      "Cluster 9\n",
      "['threat', 'possibility']\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:48:16.792515Z",
     "start_time": "2025-10-18T08:48:16.786556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    # 聚类的数量等于词/质心映射中最大的聚类索引\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # 预先分配空间\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "\n",
    "    # 遍历评论中的词语。如果该词语在词汇表中，找到它所属的聚类，并将该聚类的计数加一\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "\n",
    "    return bag_of_centroids"
   ],
   "id": "b602b1e3cb72765b",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:48:33.037496Z",
     "start_time": "2025-10-18T08:48:21.990781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters),dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews\n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters),dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1"
   ],
   "id": "e210d118d29f22f8",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:49:16.603887Z",
     "start_time": "2025-10-18T08:49:03.742425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ],
   "id": "ebcdb18f13c1049f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Paragraph Vector is better 向量平均和聚类会丢失词序，而段落向量则保留了词序信息。",
   "id": "f00517ac48e08df0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cf97a971aa8dc176"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
